{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOH9/SXWuamhMYn76HO9FGh"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Download comment letters to SEC rules"],"metadata":{"id":"d6TIskSonjO-"}},{"cell_type":"code","source":["import os\n","import requests\n","from bs4 import BeautifulSoup, NavigableString, Tag\n","from urllib.parse import urljoin\n","from openai import OpenAI\n","import csv\n"],"metadata":{"id":"XeVH24uCuzi9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","global headers\n","\n","\n","def extract_links(url):\n","    \"\"\"\n","    Fetches all unique, absolute URLs and their associated texts from the specified webpage within a specific section.\n","\n","    Args:\n","        url (str): The URL of the webpage to scrape.\n","        begin_word (str): The exact case-sensitive beginning keyword to locate the section.\n","        end_word (str): The exact case-sensitive ending keyword to locate the section.\n","\n","    Returns:\n","        list of tuples: A list of tuples where each tuple contains (URL, Link Text).\n","    \"\"\"\n","\n","\n","    try:\n","        # Send an HTTP GET request to the URL with custom headers\n","        response = requests.get(url, headers=headers)\n","        response.raise_for_status()  # Raise an exception for HTTP errors\n","    except requests.exceptions.HTTPError as http_err:\n","        print(f\"HTTP error occurred: {http_err}\")  # e.g., 403 Forbidden\n","        return []\n","    except requests.exceptions.RequestException as e:\n","        print(f\"Error fetching {url}: {e}\")\n","        return []\n","\n","    # Parse the HTML content using BeautifulSoup\n","    soup = BeautifulSoup(response.text, 'html.parser')\n","\n","\n","\n","    # Find all <a> tags within the section\n","    a_tags = soup.find_all('a')\n","\n","    links = []\n","    for tag in a_tags:\n","        href = tag.get('href')\n","        link_text = tag.get_text(strip=True)\n","        if href:\n","            # Convert relative URLs to absolute URLs\n","            absolute_url = urljoin(url, href)\n","            # Optionally, filter out URLs that are not HTTP/HTTPS\n","            if absolute_url.startswith(('http://', 'https://')):\n","                links.append((absolute_url, link_text))\n","\n","    # Remove duplicates by converting the list to a set, then back to a list\n","    unique_links = list(set(links))\n","\n","    # Apply URL filters\n","    filtered_urls = [\n","        (link, text) for (link, text) in unique_links\n","        if link.startswith(\"https://www.sec.gov/comments/\") and\n","           not (\n","               link.endswith(\".htm#main-content\") or\n","               link.endswith(\".htm#meetings\") or\n","               link.endswith(\"htm#comments\")\n","           )\n","    ]\n","\n","    return filtered_urls\n","\n","def save_links_to_file(links, filename):\n","    \"\"\"\n","    Saves the list of links and their associated texts to a CSV file.\n","\n","    Args:\n","        links (list of tuples): The list of (URL, Link Text) tuples to save.\n","        filename (str): The name of the file to save the URLs and texts in.\n","    \"\"\"\n","    try:\n","        with open(filename, 'w', encoding='utf-8', newline='') as file:\n","            writer = csv.writer(file)\n","            # Write the header\n","            writer.writerow(['URL', 'Link Text'])\n","            # Write the link data\n","            for link, text in links:\n","                writer.writerow([link, text])\n","        print(f\"Successfully saved {len(links)} links to '{filename}'.\")\n","    except IOError as e:\n","        print(f\"Error writing to file {filename}: {e}\")\n","\n","\n"],"metadata":{"id":"w0_oMyimS_Zq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["rule = 's70715'\n","\n","target_url = \"https://www.sec.gov/comments/s7-07-15/{}.htm\".format(rule)\n","\n","extracted_links = extract_links(target_url)\n"],"metadata":{"id":"yzsE4JnUWTss"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","\n","def get_file_extension(url, response):\n","    \"\"\"\n","    Determines the file extension based on the URL or the response's Content-Type header.\n","    Supports PDF and HTML files.\n","    \"\"\"\n","    # Attempt to extract extension from URL\n","    path = urlsplit(url).path\n","    ext = os.path.splitext(path)[1].lower()\n","    if ext in ['.pdf', '.htm', '.html']:\n","        return ext if ext != '.htm' else '.html'  # Normalize to .html\n","\n","    # Fallback: Determine from Content-Type header\n","\n","\n","\n","    content_type = response.headers.get('Content-Type', '').lower()\n","    if 'application/pdf' in content_type:\n","        return '.pdf'\n","    elif 'text/html' in content_type:\n","        return '.html'\n","    else:\n","        return ''\n","\n","\n","def sanitize_filename(filename, max_length=100):\n","    \"\"\"\n","    Removes or replaces characters that are invalid in filenames.\n","    Limits the filename length to max_length characters.\n","    Appends a unique hash to avoid conflicts.\n","    \"\"\"\n","    # Define valid characters\n","    valid_chars = \"-_.() abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789\"\n","\n","    # Remove invalid characters\n","    sanitized = ''.join(c for c in filename if c in valid_chars)\n","\n","    # Truncate the filename if it exceeds max_length - length of hash\n","    hash_suffix = hashlib.md5(filename.encode()).hexdigest()[:6]  # 6-character hash\n","    if len(sanitized) > (max_length - 7):  # 6 for hash, 1 for underscore\n","        sanitized = sanitized[:max_length - 7].rstrip() + \"_\"\n","\n","    sanitized = f\"{sanitized}{hash_suffix}\"\n","\n","    return sanitized or \"default_filename\"\n","\n","\n","def download_file(name, url):\n","    \"\"\"\n","    Downloads a file from a URL and saves it with the specified name and correct extension.\n","    Handles PDF and HTML files.\n","    \"\"\"\n","    try:\n","        # Initiate the GET request\n","        with requests.get(url, stream=True, timeout=15, headers = headers) as response:\n","            response.raise_for_status()  # Raise an error for bad status codes\n","\n","            # Determine the file extension\n","            ext = get_file_extension(url, response)\n","            if not ext:\n","                print(f\"⚠️  Warning: Could not determine the file extension for '{name}'. Skipping download.\")\n","                return\n","\n","            # Sanitize the filename\n","            safe_name = sanitize_filename(name)\n","\n","\n","            # Get total size in bytes for progress bar\n","            total_size = int(response.headers.get('content-length', 0))\n","            if total_size == 0:\n","                print(f\"⚠️  Warning: Cannot determine the size of '{name}'. Downloading without progress bar.\")\n","                total_size = None  # tqdm will handle it\n","\n","            # Download the file with a progress bar\n","            with open(safe_name, 'wb') as file, tqdm(\n","                total=total_size, unit='iB', unit_scale=True,\n","                desc=f\"Downloading {safe_name}{ext}\", leave=False\n","            ) as progress_bar:\n","                for chunk in response.iter_content(chunk_size=1024):\n","                    if chunk:  # Filter out keep-alive chunks\n","                        file.write(chunk)\n","                        progress_bar.update(len(chunk))\n","\n","\n","    except requests.exceptions.HTTPError as http_err:\n","        print(f\"❌  HTTP error occurred for '{name}': {http_err}\")\n","    except requests.exceptions.ConnectionError as conn_err:\n","        print(f\"❌  Connection error occurred for '{name}': {conn_err}\")\n","    except requests.exceptions.Timeout as timeout_err:\n","        print(f\"❌  Timeout error occurred for '{name}': {timeout_err}\")\n","    except requests.exceptions.RequestException as req_err:\n","        print(f\"❌  An error occurred for '{name}': {req_err}\")\n","    except Exception as e:\n","        print(f\"❌  An unexpected error occurred for '{name}': {e}\")\n","\n","\n"],"metadata":{"id":"5QzRHkehawOL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# Iterate through each tuple in extracted_links\n","for index, (url, name) in enumerate(extracted_links, start=1):\n","    if not url or not name:\n","        print(f\"⚠️  Skipping entry {index}: Missing 'Link' or 'Name'.\")\n","        continue\n","\n","    download_file(name, url)\n","\n","print(\"All downloads completed.\")\n"],"metadata":{"id":"Kf-TD-Csaxq2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1734695690642,"user_tz":0,"elapsed":92789,"user":{"displayName":"Hao","userId":"17979187051427251278"}},"outputId":"ff5d0b43-d34f-42bd-ef25-bb93522ad7d0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["                                                                                                                                                                 "]},{"output_type":"stream","name":"stdout","text":["All downloads completed.\n"]},{"output_type":"stream","name":"stderr","text":["\r"]}]}]}